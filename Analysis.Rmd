---
title: "Analysis Rattus"
author: "Julien Christophe Piquet"
date: "24/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=F)
```

```{r folder}
folder<-"E:/Research/Projects/A_2021_CSIC_rats/Research/Data"
```

# **Analysis with combined data**

## **Analysis of captures**

We first analyze the number of captures and the number of recaptures per site.

```{r capture and recaptures}
library(readxl)
captfile<-read_xlsx(paste0(folder,"/data_secr.xlsx")) # reading capture data

# Checking if individuals were captured in multiple sites

table_capt<-table(captfile$id,captfile$site) # creating a table with the number of captures per site and individual
sites_ind<-apply(table_capt, 1, function(c)sum(c!=0)) # retaining only rows !=0 (i.e. those representing captures)
mean(sites_ind) # checking individuals only appear in one site

#Calculating number of captures and recaptures

length(captfile$id) # number of captures
length(unique(captfile$id)) # number rats
table_captures<-as.matrix(table(captfile$id))
colnames(table_captures)<-"n_captures"
mean(table_captures)
sd(table_captures)
max(table_captures)
table_captures<-as.data.frame(table_captures)
table_captures$id<-row.names(table_captures) # including rat id
table_captures<-merge(table_captures,captfile,by="id",all.x=T) # incorporation of site information
table_captures<-table_captures[,c(1,2,7)] # retaining id, capture number and site
table_captures<-table_captures[!duplicated(table_captures$id),] # removing duplicates (i.e. recaptures, which are included)
table(table_captures$site) # number of individuals per site
sum<-aggregate(table_captures$n_captures,by=list(table_captures$site),FUN=sum) # total number of (re)captures per site
mean<-aggregate(table_captures$n_captures,by=list(table_captures$site),FUN=mean) # average number of (re)captures per site
sd<-aggregate(table_captures$n_captures,by=list(table_captures$site),FUN=sd) # SD pf the number of (re)captures per site
table<-cbind(as.data.frame(table(table_captures$site)),as.data.frame(sum)[,2],as.data.frame(mean)[,2],as.data.frame(sd)[,2])
colnames(table)<-c("site","No_ind","No_captures","mean","sd")

write.csv(table,file=paste0(folder,sep="/","table_summary.csv"))
```

## **Analysis of movements**

We create the data from the capture history and the trap locations.

```{r data formation combined data}
library(readxl)

captfile<-read_xlsx(paste0(folder,"/data_secr.xlsx")) # reading capture data
captfile$session<-captfile$site # seven session (one per site)
captfile<-captfile[,-c(5,6)] # removing unnecessary columns
write.table(captfile,file=file.path(paste0(folder,"/captfile.txt")),sep="\t",quote=F,row.names = F)
remove(captfile)
trapfile<-read_xlsx(paste0(folder,"/data_secr.xlsx"),sheet="trap_location")
trapfile_list<-split(trapfile,trapfile$site) # splitting per site
trapfile_list<-lapply(trapfile_list,"[",-5) # we remove the site information
mapply(function(x,y) write.table(x,file=paste0(folder,"/trap_data/",y,".txt",sep=""),row.names = F,quote=F,sep="\t"),x=trapfile_list,y=names(trapfile_list))
```

We use the previously formatted data to create the secr database.

```{r secr analysis combined data}
library(secr)
secr_data<-read.capthist(captfile=paste0(folder,"/captfile.txt"),trapfile=list.files(paste0(folder,"/trap_data"),pattern=".txt",full.names = T),detector="single",fmt=c("trapID"),noncapt="NONE",noccasions = 9,verify=T,skip=1,binary.usage=F) # creating capthist data
rm(list=setdiff(ls(),c("secr_data","folder")))
```

We calculate the home range statistics, with particular interest in the moves.

```{r home range statistics combined data}

# Calculating home range statistics

dbar<-dbar(secr_data)
moves<-moves(secr_data)
mdmm<-MMDM(secr_data)
rpsv<-RPSV(secr_data)
rpsv<-as.matrix(cbind(unlist(rpsv)))
rpsv<-rpsv[is.finite(rpsv),]
width<-max(rpsv,na.rm = T)*4 # establishing the maximum width
```

We analyze the distance moved per individual between successive captures.

```{r moves}
library(car)
library(onewaytests)

table_moves<-unlist(moves)
table_moves<-as.data.frame(table_moves)
colnames(table_moves)<-"move"
table_moves$site<-substr(row.names(table_moves),start=1,stop=3) # including site
table_moves$move<-as.numeric(table_moves$move)

# Exploration of data

qqnorm(table_moves$move) # not quite normal
shapiro.test(table_moves$move)
leveneTest(table_moves$move,group=table_moves$site) # heterogeneity
welch.test(move~site,data=table_moves,rate=0.1) # no difference among sites
mean(table_moves$move)
sd(table_moves$move)
max(table_moves$move)
aggregate(table_moves$move,by=list(table_moves$site),mean)
aggregate(table_moves$move,by=list(table_moves$site),sd)
rm(list=setdiff(ls(),c("secr_data","rpsv","width","folder")))
```

## **Analysis of density**

We begin the calculation of density by setting the buffer width.

```{r definition of width combined data}

# Calculating starting values

start<-secr.fit(secr_data,start=list(g0=0.1,sigma=max(rpsv,na.rm = T)),buffer=width,verify=F,detectfn = 0,biasLimit=0.01,method="Nelder-Mead",CL=T,trace=F)

## Checking buffer width

buffer_width<-esa.plot(start,max.buffer = width*2,detectfn = 0,noccasions = 9,session = c(names(secr_data))) # fitting density against buffer width
buffer_width<-lapply(buffer_width,function(x) x$buffer[which.min(x$density)]) # finding for each site-session combination the buffer width were density stabilizes
max_width<-max(unlist(buffer_width)) # maximum value of buffer width over all site-session combinations for which density stabilizes
rm(list=setdiff(ls(),c("secr_data","max_width","start","folder")))
```

Once buffer width is set, we determine the best spatial detection function using AICc, and examine the potential occurrence of learned responses from R. rattus.

```{r model density estimation with combined data}
memory.limit(80000)

# Halfnormal model

secr.halfnormal<-secr.fit(secr_data,buffer=max_width,detectfn = 0,verify=F,start=start,method="Nelder-Mead",trace=F,CL=T)

# Negative exponential model

secr.negexp<-secr.fit(secr_data,buffer=max_width,detectfn = 2,verify=F,start=start,method="Nelder-Mead",trace=F,CL=T)

# Hazard rate model

secr.hazrat<-secr.fit(secr_data,buffer=max_width,detectfn = 1,verify=F,start=start,method="Nelder-Mead",trace=F,CL=T)

list_models<-list(secr.halfnormal,secr.negexp,secr.hazrat)

AIC_table_detecfn<-lapply(list_models,function(x) AIC(x)) # calculating AIC
AIC<-sapply(AIC_table_detecfn,"[[","AIC")
AICc<-sapply(AIC_table_detecfn,"[[","AICc")
model<-sapply(AIC_table_detecfn,"[[","model")
detection_function<-sapply(AIC_table_detecfn,"[[","detectfn")
AIC_table_detecfn<-cbind(model,detection_function,AIC,AICc)
AIC_table_detecfn # halfnormal is the selected one

# Checking learned responses

secr.halfnorm.learned<-secr.fit(secr_data,model=list(g0~b),buffer=max_width,detectfn = 0,verify=F,start=start,method="Nelder-Mead",trace=F,CL=T)

AIC(secr.halfnorm.learned) # greater AIC than without learned responses

final_model<-secr.halfnormal

density<-derived(final_model,se.esa=F,se.D=T)

density_estimate<-lapply(density,"[[","estimate")
density_estimate<-lapply(density_estimate,"[",2)
density_se<-lapply(density,"[[","SE.estimate")
density_se<-lapply(density_se,"[",2)
cvd<-lapply(density,"[[","CVD")
cvd<-lapply(cvd,"[",2)

table_density<-cbind(do.call(rbind,density_estimate),do.call(rbind,density_se),do.call(rbind,cvd))

colnames(table_density)<-c("density","se","cvd")

write.table(table_density,file=paste0(folder,"/density_data.csv"),sep=",",row.names = T,col.names = T)

table_density<-as.data.frame(table_density)
mean(table_density$density)
sd(table_density$density)

table_density$location<-c(rep("TENO",5),rep("GUAZA",2))

# Comparison of density between Teno and Guaza

qqnorm(table_density$density) # not quite normal
shapiro.test(table_density$density)
leveneTest(table_density$density,table_density$location) # no heterogeneity
kruskal.test(table_density$density,table_density$location)
aggregate(table_density$density,list(table_density$location),mean)
aggregate(table_density$density,list(table_density$location),sd)
```

```{r area used in each site}

library(readxl)
library(adehabitatHR)
library(raster)

area<-read.csv(file=paste0(folder,sep="/","trap_location.csv"),sep=";")

# Separating the effort column 

days<-lapply(c(1,3,5,7,9,11,13,15,17),function(x) substr(area$effort,start=x,stop=x)) 
days<-do.call(cbind,days)
colnames(days)<-lapply(seq(1,9,1),function(x) paste0("day",x))
area<-cbind(area,days)
remove(days)

# Creating a separate spatialpoints dataframe per site and days

area_list<-lapply(area[,6:14],function(x) subset(area,x==1))
date<-mapply(function(x,y) rep(x,y),x=names(area_list),y=lapply(area_list,nrow))
area_table<-do.call(rbind,area_list)
date<-unlist(date)
area<-cbind(area_table,date)
area<-area[,-c(6:14)]
area_list<-split(area,list(area$site,area$date),drop=T)
area_list<-lapply(area_list,function(x) SpatialPoints(x[,2:3],proj4string = CRS("+proj=utm +zone=28 +datum=WGS84 +units=m +no_defs")))

# Calculating the area of each site 

area_list<-lapply(area_list,function(x) mcp(x,percent=100)) # mcp in ha
area_list<-lapply(area_list,"[[","area")
area_table<-do.call(rbind,area_list)
area_table<-as.data.frame(area_table)
area_table$site<-substr(row.names(area_table),start=1,stop=3)
area_table$date<-substr(row.names(area_table),start=5,stop=length(row.names(area_table)))
colnames(area_table)[1]<-"area"
mean_area<-tapply(area_table$area,area_table$site,mean)
sd_area<-tapply(area_table$area,area_table$site,sd)
area_table<-cbind(mean_area,sd_area)
area_table
rm(list = setdiff(ls(),"folder"))
```


# **Correlation with rat abundance and frequency of occurrence**

```{r correlations}

#Re-uploading the tables

library(readxl)
density_table<-read.csv(file=paste0(folder,"/density_data.csv"),header=TRUE,sep=",")
abundance_fo<-read_excel(path=paste0(folder,"/abundance_fo.xlsx"))
density_table<-cbind(density_table,abundance_fo[,2:length(abundance_fo)])

# Plotting

plot(density_table$density,density_table$abundance)
with(density_table,cor.test(density,abundance,method="spearman"))
plot(density_table$density,density_table$fo_exc)
text(density_table$density,density_table$fo_exc,density_table$site)
plot(density_table$density,density_table$fo_ind)
text(density_table$density,density_table$fo_ind,density_table$site)
with(density_table,cor.test(density,fo_exc,method="spearman"))
with(density_table,cor.test(density,fo_ind,method="spearman"))
```

# **Correlation with lizard census**

```{r lizard census}
library(car)

# Calculating reproductive vs. non-reproductive individuals

## Non-reproductive (juveniles+average-sized lizards)

density_table$non_reproductive<-density_table$juv_num+density_table$ave_num

## Reproductive individuals (males+females+non-identified individuals)

density_table$non_identified<-density_table$liz_num-apply(density_table[,11:14],1,sum)
density_table$reproductive<-density_table$fem_num+density_table$mal_num+density_table$non_identified

# Calculating population trend

density_table$population_trend<-(density_table$diff_liz/density_table$liz_past)*100

# Analysis

correlations_dens<-lapply(density_table[,c(7,15,16,18,19)],function(x) with(density_table,cor.test(density,x,method="spearman")))
correlations_dens<-cbind(do.call(rbind,lapply(correlations_dens,"[[","estimate")),do.call(rbind,lapply(correlations_dens,"[[","p.value")))
write.csv(correlations_dens,file=paste0(folder,sep="/","correlation_dens.csv"))

correlations_fo_exc<-lapply(density_table[,c(7,15,16,18,19)],function(x) with(density_table,cor.test(fo_exc,x,method="spearman")))
correlations_fo_exc<-cbind(do.call(rbind,lapply(correlations_fo_exc,"[[","estimate")),do.call(rbind,lapply(correlations_fo_exc,"[[","p.value")))
write.csv(correlations_fo_exc,file=paste0(folder,sep="/","correlation_fo_exc.csv"))

# Checking relationship between the number of lizards/rat density and site area

lapply(density_table[,c(1,4,7)],function(x) with(density_table, cor.test(x,area,method="spearman")))

write.csv(density_table,file=paste0(folder, sep="/","density_table.csv"),col.names=T)
```

