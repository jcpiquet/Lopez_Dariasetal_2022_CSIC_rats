---
title: "Analysis"
author: "Julien C. Piquet"
date: "2024-01-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **PACKAGES, FOLDERS AND DATA CREATION**

We first load all packages needed.

```{r packages}
library(pacman)

p_load(readxl,dplyr,lubridate,tidyr,secr,car,onewaytests,VGAM)
```

Now we set the folders were data will be stored.

```{r folder}
folder<-"E:/Research/Projects/A_2021_CSIC_rats/Research/Data/"

folder_files<-"E:/Research/Projects/A_2021_CSIC_rats/Research/Data/files"
```

We create the data for the analysis.

```{r data}
data<-read_xlsx(
  paste0(folder,"/data.xlsx"),
  sheet="data") # reading data

# FACTORS

data[,c(1,5:9,12:ncol(data))]<-lapply(data[,c(1,5:9,12:ncol(data))],as.factor)

# DATE AND TIME

data$date_layout<-as.POSIXct(data$date_layout)

year(data$time_start)<-year(data$date_layout)

year(data$time_end)<-year(data$date_layout)

month(data$time_start)<-month(data$date_layout)

month(data$time_end)<-month(data$date_layout)

day(data$time_start)<-day(data$date_layout)

day(data$time_end)<-day(data$date_layout)+1

# NATURAL PROTECTED AREAS

data$npa<-NA

data[data$site=="par"|data$site=="nam",]$npa<-"guaza"

data[is.na(data$npa),]$npa<-"teno"
```

# **BASIC TRAPPING DATA**

We obtain basic trapping data.

```{r basic trapping data}

# TRAPS PER SITE AND DAY

trap_count<-as.data.frame(
  count(data,site,day))

round(
  mean(trap_count[,3]),
  2)
round(
  sd(trap_count[,3]),
  2)

remove(trap_count)

# TRAPPING EFFORT

effort<-unique(data[,c(1,3:4)]) # removing duplicates

mean( # calculating the average number of hours spent trapping
  as.numeric( # converting to numeric to allow mean calculation
    effort$time_end-effort$time_start),
  na.rm=T)*9*7 # multiplying per day and site

# NUMBER OF RATS AND FECES

table(data$outcome) # number of rats

table(data[data$outcome!="mus",]$feces) # number of feces, after removing feces from Mus musculus

## Number of individuals

length(unique(data[complete.cases(data$id),]$id))

## Percentage of captures by marked animals

nrow(data[data$outcome=="rat"|data$outcome=="escaped",])/nrow(data[data$outcome=="rat"|data$outcome=="escaped"|data$outcome=="dead",]) *100 

## Captures per trapping session

table(data$outcome,data$session)

# RAT DATA

## Captures per sex category

count(data[data$outcome=="rat"| data$outcome=="dead" ,],sex)

## Individual recaptures

recaptures<-count(data,id) # counting the number of captures per individual

recaptures<-recaptures[complete.cases(recaptures$id),] # removing empty cells 

round(mean(recaptures$n),2) # average number of captures

round(sd(recaptures$n),2)

max(recaptures$n) # maximum number of captures

### Checking recaptures occurred within the same site
 
recaptures<-as.data.frame(table(data$id,data$site)) # retrieving number of captures of each individual in each site

nrow(recaptures[recaptures$Freq>0,]) # checking the number of rows equals the number of individuals, which means each individual is only present in one site

remove(recaptures)

# BY-CATCHES

table(data$outcome)
```

# **LIZARD DETECTION**

We analyze the lizard detection.

```{r lizard detection}

# PERCENTAGE OF FECES WITH LIZARDS

(table(data[data$outcome!="mus",]$detection_lizards,
       data[data$outcome!="mus",]$feces)[2,2]/
   sum(table(data[data$outcome!="mus",]$detection_lizards,
         data[data$outcome!="mus",]$feces)[,2]))*100 

# INDIVIDUALS THAT CONSUMED LIZARDS

lizards<-data[complete.cases(data$detection_lizards),]

lizards<-lizards[lizards$outcome!="mus",]

lizards<-lizards[lizards$detection_lizards==1,]

length(unique(lizards$id))

table(lizards$sex) # checking how many adults and juveniles consumed lizards

max(count(lizards,id)[,2]) # maximum number of feces from a single individual in which lizards were detected

min(count(lizards,id)[,2]) # maximum number of feces from a single individual in which lizards were detected

# DETECTIONS PER SITE

table(lizards$site)

# CALCULATION OF FOEXC PER SITE

fo<-count(data,site,detection_lizards) 

fo<-fo[fo$detection_lizards==1,] # retaining only detections

fo<-fo[complete.cases(fo),] # removing empty cells

fo<-fo[,c(1,3)]

feces<-count(data[data$outcome!="mus",],feces,site) # number of feces per site, after removing Mus musculus captures

feces<-feces[feces$feces=="yes",] # retaining only feces

feces<-feces[complete.cases(feces),] # removing empty cells

feces<-feces[,2:3]

colnames(feces)[2]<-"feces" 

fo$feces<-feces$feces # merging both datasets to calculate FO

fo$foexc<-(fo$n/fo$feces)*100 # FO calculation per site

colnames(fo)[2]<-"detection_lizards"

(sum(fo$detection_lizards)/sum(fo$feces))*100 # general FO

# CALCULATION OF FOIND PER SITE

rat_detection_lizard<-count(data[data$outcome!="mus",],site,id,detection_lizards) # counting without Mus musculus

rat_detection_lizard<-rat_detection_lizard[complete.cases(rat_detection_lizard),] # removing empty cells

rat_detection_lizard<-rat_detection_lizard[rat_detection_lizard$detection_lizards==1,] # retaning only positive counts

rat_detection_lizard<-sapply(split(rat_detection_lizard,
                                   rat_detection_lizard$site),
                             nrow) # number of detections

rat_detection_lizard<-as.data.frame(rat_detection_lizard) # as.data.frame to merge easily

rats<-count(data,site,outcome,id)

rats<-rats[rats$outcome=="rat" | rats$outcome=="escaped"|rats$outcome=="dead", ] # number off rats captured

rats<-sapply(split(rats,rats$site),
             function(x) length(unique(x$id))) # distinct individuals

rat_detection_lizard$rats<-rats # merging for FO calculation

rat_detection_lizard$fo_ind<-(rat_detection_lizard$rat_detection_lizard/rat_detection_lizard$rats)*100 # FOind calculation per site

fo$fo_ind<-rat_detection_lizard$fo_ind

(sum(rat_detection_lizard$rat_detection_lizard)/sum(rat_detection_lizard$rats))*100 # general FO

# DETECTIONS ACROSS SEASONS

count(lizards,session,detection_lizards) # number of detections per session

lizards<-data[complete.cases(data$detection_lizards),] # removing incomplete data

lizards<-lizards[lizards$detection_lizards==1,]

tapply(lizards,lizards$session,function(x)
  length(unique(x$id))) # number of individuals tested positive

# DETECTIONS IN TENO AND GUAZA

table(lizards$npa)

tapply(lizards,lizards$npa,function(x)
  length(unique(x$id))) # number of individuals tested positive

rm(list=setdiff(ls(),c("folder","folder_files","data","fo")))
```

# **CPUE ANALYSIS**

This section intends to calculate CPUE.

```{r cpue}

traps<-count(data,site,day,trap_type) # calculating number of traps of each type

sprung_traps<-count(data[data$active=="no",],site,day,trap_type) # calculating sprung_traps

sprung_traps$code<-with(sprung_traps, paste0(site, day, trap_type)) # creating a code to merge datasets

traps$code<-with(traps,paste0(site, day, trap_type))

cpue<-merge(traps,sprung_traps,by="code",all.x=T) # merging

cpue<-cpue[,c(1:5,9)] # removing duplicate columns

colnames(cpue)<-c("code","site","day","trap_type","trap_no","sprung_traps")

cpue[is.na(cpue$sprung_traps),]$sprung_traps<-0 # changing empty cells to 0

remove(traps,sprung_traps)

rat_captures<-count(data[data$outcome=="rat",],site,day,trap_type,outcome) # counting rat capures

dead_captures<-count(data[data$outcome=="dead",],site,day,trap_type,outcome) # dead individuals

rat_captures$code<-with(rat_captures,paste0(site, day, trap_type)) # merging code

dead_captures$code<-with(dead_captures,paste0(site, day, trap_type)) # merging code

cpue<-merge(cpue,rat_captures,by="code",all.x=T) # merging 

cpue<-merge(cpue,dead_captures,by="code",all.x=T) # merging 

cpue<-cpue[,c(1:6,11,16)] # removing duplicate columns

colnames(cpue)[c(2,3,4,7,8)]<-c("site","day","trap_type","rats","dead_rats") 

cpue[, 7:8][is.na(cpue[, 7:8])] <- 0 # replacing empty cells

sum(cpue$rats) # checking captures coincide

sum(cpue$dead_rats)

cpue$total_captures<-cpue$rats+cpue$dead_rats # summing dead individuals

cpue$cpue<-cpue$total_captures/(cpue$trap_no-cpue$sprung_traps) # cpue calculation

# COMPARING CPUE AMONG TRAPS AND SITES

## Basic information

aggregate(cpue~trap_type,data=cpue,mean) 

cpue_site<-cbind(aggregate(cpue~site,data=cpue,mean),
                 sd=aggregate(cpue~site,data=cpue,sd)[,2]
                 ) 

cpue_site[,2:3]<-lapply(cpue_site[,2:3],function(x) round(x,3))

cpue_site$tom<-aggregate(cpue~site,data=cpue[cpue$trap_type=="tom",],mean)[,2]

cpue_site$ave<-aggregate(cpue~site,data=cpue[cpue$trap_type=="ave",],mean)[,2]

cpue_site$sma<-aggregate(cpue~site,data=cpue[cpue$trap_type=="sma",],mean)[,2]

mean(cpue$cpue)

sd(cpue$cpue)

# COMPARING THE NUMBER OF TRAPS AMONG SITES

lapply(split(data,data$trap_type),function(x)
  chisq.test( # chi-square of trap number distribution among sites and days
    table(x$site,x$day),
    simulate.p.value = T, # simulated to overcome low expected values
    B=10000
  ))

chisq.test( # chi-square repeated removing sessions in which FIXMAN traps were not used
  table(droplevels(data[data$trap_type=="ave" & as.numeric(data$day)<4,]$site),
        droplevels(data[data$trap_type=="ave" & as.numeric(data$day)<4,]$day)),
  simulate.p.value = T, # simulated to overcome low expected values
  B=10000
)

remove(dead_captures,rat_captures,cpue)
```

# **DENSITY ANALYSIS**

We  begin the section to obtain rat density. First, we upload the data that were prepared or this section.

```{r data density}

captfile<-read_xlsx(
  paste0(folder,"/data.xlsx"),
  sheet="capture_data") # reading capture data

trapfile<-read_xlsx(
  paste0(folder,"/data.xlsx"),
  sheet="traps")

captfile$session<-captfile$site # seven session (one per site)

captfile<-captfile[,-c(3,6:7)] # removing id_day, site and session_site

colnames(captfile)[3]<-"occasion" # changing names of the columns
```

Since data were prepared manually, we first check that there are no errors.

```{r data verification}

# CAPTURES COMPARISON

captures<-count(data[data$outcome=="rat"|data$outcome=="dead",],outcome,day,site) # counting captures in the general data (escaped individuals not included)

captures_captfile<-count(captfile,session,occasion) # counting captures in the captfile data (escaped individuals not included)

## Ordering data the same manner

captures<-captures[order(captures$site,captures$day,captures$outcome),]

captures_captfile<-captures_captfile[order(captures_captfile$session,captures_captfile$occasion),]

# Comparing sites and capture numbers

captures<-cbind(captures,captures_captfile) # merging both datasets

captures$site_verification<-apply(captures,1,function(x) setequal(captures$site,captures$session)) # verifying for each row that site names coincides

nrow(captures[captures$site_verification=="FALSE",]) # number of rows where site names are not equal

mean(captures[,4]-captures[,7]) # comparing captures

remove(captures,captures_captfile)

# CHECKING TRAPS

## Checking effort

data$utm_code<-paste0(data$utm_x,"_",data$utm_y) # creating the utm code that identifies each trap location

traps<-count(data[data$active=="yes",],utm_code,site,day) # counting active traps per day and site

mapply(function(x,y) x[,2]-y[,2], # automatically subtracting counts 
       x=lapply(trapfile[,6:14],
                function(z) aggregate(z~site, # aggregating in a list per day and site
                                      data=trapfile,
                                      sum)),
       y=lapply(seq(1,9,1),function(x) aggregate(n~site,
                                                 traps[traps$day==x,],
                                                 sum)) # aggregating in a list per day and site
)

## Checking trap type

traps<-count(data[data$active=="yes",],site,day,trap_type) # counting active traps per day, site and type

colnames(trapfile)[16:24]<-paste0("trap_type","_",seq(1,9,1))  # assigning a name to trap_type columns

trapfile[,16:24]<-lapply(trapfile[,16:24], as.factor) # changing them to factor

tables<-lapply(trapfile[,16:24],function(x) table(trapfile$site,x)) # per site

tables<-lapply(tables,as.data.frame) # transforming to dataset

tables<-lapply(tables,function(x) x[order(x$Var1,x$x),]) # re-ordering datasets

tables_traps<-lapply(split(traps,traps$day),function(x) aggregate(n~trap_type+site,x,sum)) # calculting the number of traps per type and day

tables<-lapply(tables_traps,function(x) x[order(x$site,x$trap_type),]) # re-ordering 

mapply(function(x,y) x[,3]-y[,3], # subtracting
       x=tables,
       y=tables_traps)

rm(list=setdiff(ls(),c("captfile","trapfile","data","fo","folder","folder_files","cpue_site")))
```

We now prepare the data for the analysis.

```{r data prepr}

write.table(captfile[,-5],
            file=file.path(
              paste0(folder_files,"/captfile.txt")),
            sep="\t",quote=F,row.names = F)

trapfile<-trapfile[,c(5,2:3,15,25,4)] # re-ordering and keeping only point, utms, effort and trap_type, plus site info

trapfile_list<-split(trapfile,trapfile$site) # splitting per site

trapfile_list<-lapply(trapfile_list,"[",-6) # we remove the site information

mapply(function(x,y) # writing separate files per session (site)
  write.table(x,file=
                paste0(folder_files,"/traps/",y,".txt",sep=""),
              row.names = F,quote=F,sep="\t"),
  x=trapfile_list,y=names(trapfile_list))

rm(list=setdiff(ls(),c("data","fo","folder","folder_files","cpue_site")))
```

The previous data now have to be integrated within secr.

```{r secr data}

secr_data<-read.capthist(captfile=
                           paste0(folder_files,"/captfile.txt"),
                        trapfile=list.files(
                              paste0(folder_files,"/traps"),
                              pattern=".txt",full.names = T),
                        detector="single",
                        fmt=c("trapID"),
                        noncapt="NONE",
                        noccasions = 9,
                        verify=T,
                        skip=1,
                        binary.usage=F) # creating capthist data

for (i in 1:7) timevaryingcov(traps(secr_data[[i]])) <- list(type = 1:9) # specifying the type of trap for each session and day
```

We calculate the home range statistics for each dataset, with particular interest in the moves.

```{r home range statistics}

# Calculating home range statistics

dbar<-dbar(secr_data)

moves<-moves(secr_data)

mmdm<-MMDM(secr_data)

rpsv<-RPSV(secr_data)

home_range<-as.data.frame(
  cbind(dbar,
        mmdm,
        rpsv))

home_range<-lapply(home_range,as.numeric) # setting as numeric

width<-max(home_range$rpsv)*4 # establishing the maximum width

rm(list=setdiff(ls(),c("secr_data","folder","folder_files","width","moves","data","fo","cpue_site")))
```

We use the moves dataset to analyze potential differences in spatial behavior between sites, in case these could affect trap width.

```{r moves}

table_moves<-as.data.frame(
  unlist(moves))

colnames(table_moves)<-"moves"

table_moves$site<-substr(row.names(table_moves),start=1,stop=3) # including site

# Exploration of data

qqnorm(table_moves$moves) # not normal

leveneTest(table_moves$moves,group=table_moves$site) # heterogeneity of the variance

welch.test(moves~site,data=table_moves,rate=0.1) # no significant difference

aggregate(moves~site,table_moves,mean)

remove(moves,table_moves)
```

### **FORMAL ANALYSIS**

We begin the calculation of density by setting the buffer width.

```{r definition of width}

# STARTING VALUES

start<-secr.fit(secr_data,
                start=list(g0=0.1,
                           sigma=(width/4),# width was calculated as rpsv*4
                           buffer=width,
                           verify=F,
                           detectfn = 0,
                           biasLimit=0.01,
                           method="Nelder-Mead",
                           CL=T,
                           trace=F)
                )

# CHECKING BUFFER WIDTH

buffer_width<-esa.plot(start,# fitting density against buffer width
                       buffer = width*2,
                       detectfn = 0,
                       noccasions = 9,
                       session = c(names(secr_data))
                       )

buffer_width<-lapply(buffer_width,
                     function(x) x$buffer[which.min(x$density)]) # finding for each site-session combination the buffer width were density stabilizes for duplicate data

buffer_width<-unlist(buffer_width)

width<-max(buffer_width)
```

Once buffer width is set, we determine the actual model, including an evaluation of potential learned responses.

```{r model density estimation}

secr_model<-secr.fit(secr_data,
                        model=list(g0~type),
                        buffer=width,
                        detectfn = 0,
                        start=start,
                        verify=T,
                        method="Nelder-Mead",
                        trace=F,
                        CL=T)

secr_learned<-secr.fit(secr_data,
                                model=list(g0~b+type),
                                buffer=width,
                                detectfn = 0,
                                start=start,
                                verify=T,
                                method="Nelder-Mead",
                                trace=F,
                                CL=T)

AIC(
  secrlist(secr_model,
           secr_learned))

final_model<-secr_model

rm(list=setdiff(ls(),c("folder","folder_files","secr_data","data","final_model","fo","cpue_site")))
```

We now use the final models to derive density, extract RSE and compare results between Teno and Guaza.

```{r density analysis}

# RETRIEVING DENSITY

density<-derived(final_model,se.esa=F,se.D=T) # deriving density as a Hovitz-Thompson estimate

density_estimate<-lapply(density,"[[","estimate") # retaining only estimates

density_estimate<-lapply(density_estimate,"[",2) # retrieving only density
 
density_se<-lapply(density,"[[","SE.estimate") # retrieving SE

density_se<-lapply(density_se,"[",2) 

cvd<-lapply(density,"[[","CVD") # obtaining CVD (also called RSE) per site

cvd<-lapply(cvd,"[",2)

table_density<-cbind(do.call(rbind,density_estimate),do.call(rbind,density_se),do.call(rbind,cvd)) # binding everything together to have density dataset

colnames(table_density)<-c("density","se","rse")

write.table(table_density,file=paste0(folder,"/density_data.csv"),sep=",",row.names = T,col.names = T)

table_density<-as.data.frame(table_density)

mean(table_density$density)

sd(table_density$density)

table_density$site<-rownames(table_density) # site names are in the rows, extracting it to ease merging

rownames(table_density)<-1:nrow(table_density)
  
table_density$npa<-NA # putting protected natural areas names

table_density[table_density$site=="par"|table_density$site=="nam",]$npa<-"guaza"

table_density[is.na(table_density$npa),]$npa<-"teno"

## Comparison of density between Teno and Guaza

qqnorm(table_density$density) # quite normal

shapiro.test(table_density$density)

leveneTest(table_density$density,table_density$npa) # no heterogeneity

kruskal.test(table_density$density,table_density$npa)

aggregate(table_density$density,list(table_density$npa),mean)

aggregate(table_density$density,list(table_density$npa),sd)
```

# **RELATING DENSITY TO ABUNDANCE AND LIZARD CONSUMPTION**

We now evaluate the relationship of rat density in each location with rat CPUE and lizard consumption indices.

```{r density and abundance related to lizard consumption}

site_data<-merge(table_density,cpue_site,by="site") # merging density data to CPUE

site_data<-merge(site_data,fo,by="site")

lapply(site_data[,c(6,13:14)],function(x) # correlation analyses between density, rat abundance and lizard consumption
  with(site_data,cor.test(x,density,method="spearman")))
```

# **IMPACT ON LIZARD POPULATIONS**

We finally evaluate how density is related to lizard population data.

```{r lizard population data}

lizard<-read_xlsx(
  paste0(folder,"/lizards.xlsx")) # reading data

site_data<-merge(site_data,lizard,by="site") # merging site data and lizard pop data

site_data$reprod<-site_data$mal_num+site_data$fem_num # calculating number of reproductive individuals

site_data$nonreprod<-site_data$juv_num+site_data$ave_num # calculating number of non-reproductive individuals

site_data$poptrend<-(site_data$diff_liz)*100/site_data$liz_past # calculating population trend

lapply(site_data[,c(15,23,26:28)],function(x)
  with(site_data,cor.test(density,x,method="spearman")))

lapply(site_data[,c(15,23,26:28)],function(x)
  with(site_data,cor.test(foexc,x,method="spearman")))

write.table(site_data,file=paste0(folder,"/","site_data.csv"),sep=";")
```

# **SUMMARY TABLE**

We retrieve the data for the summary table.

```{r summary table}

summary_table<-count(data,outcome,site) # summary table for the manuscript

summary_table<-summary_table[summary_table$outcome=="rat",] # number of rat captures per site

feces<-count(data[data$outcome!="mus",],feces,site) # number of feces per site, after removing Mus musculus captures

feces<-feces[feces$feces=="yes",]

feces<-feces[complete.cases(feces),]

feces<-feces[,2:3]

colnames(feces)[2]<-"feces"

marked_rats<-split(data,data$site)

marked_rats<-lapply(marked_rats,function(x) x[complete.cases(x$id),])

marked_rats<-lapply(marked_rats, function(x) length(unique(x$id)))

marked_rats<-as.data.frame( # number of different rat individuals
  sapply(split(data,data$site),
         function(x)
           length(unique(x$id))))

dead<-count(data[data$outcome=="dead",],outcome,site) # dead individuals

colnames(dead)[3]<-"dead"

dead<-dead[,2:3]

escaped<-count(data[data$outcome=="escaped",],outcome,site) # escaped individuals

colnames(escaped)[3]<-"escaped"

escaped<-escaped[,2:3]

# RECAPTURES

recaptures<-count(data,site,id) 

recaptures<-recaptures[complete.cases(recaptures$id),] # removing empty cells

recaptures<-recaptures[recaptures$n>1,] # retaining only individuals captured at least twice

recaptures<-aggregate(n~site,recaptures,sum) # summing recaptures per site

colnames(recaptures)[2]<-"recaptures"

# DETECTION LIZARDS

detection_lizards<-count(data,site,detection_lizards) 

detection_lizards<-detection_lizards[detection_lizards$detection_lizards==1,] # retaining only detections

detection_lizards<-detection_lizards[complete.cases(detection_lizards),] # removing empty cells

detection_lizards<-detection_lizards[,c(1,3)]

colnames(detection_lizards)[2]<-"detection_lizards"

# NUMBER OF INDIVIDUALS THAT CONSUMED LIZARDS

rat_detection_lizard<-count(data,site,id,detection_lizards) 

rat_detection_lizard<-rat_detection_lizard[complete.cases(rat_detection_lizard),]

rat_detection_lizard<-rat_detection_lizard[rat_detection_lizard$detection_lizards==1,]

rat_detection_lizard<-sapply(split(rat_detection_lizard,rat_detection_lizard$site),nrow)

rat_detection_lizard<-as.data.frame(rat_detection_lizard) # as.data.frame to merge easily

# MERGING

summary_table<-merge(summary_table,escaped,by="site",all.x=T)

summary_table<-merge(summary_table,dead,by="site",all.x=T)

summary_table<-cbind(summary_table,marked_rats,recaptures[,2],detection_lizards[,2],rat_detection_lizard,feces[,2])

summary_table<-summary_table[,-c(2)]
  
colnames(summary_table)<-c("site","rats","escaped","dead","marked","recaptures","detection_lizatds","rat_detection_lizards","feces")

summary_table<-merge(summary_table,site_data,by="site")

summary_table$rats<-apply(summary_table[,2:4],1,function(x) sum(x,na.rm = T))

write.table(summary_table,file=paste0(folder,"/","summary_table.csv"),sep=";")
```



